{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Processing\n",
    "\n",
    "This notebook collects and saved the attacking reults from multiple runs into a single file.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Conducted the Model Training and the Model Inversion metrics to produce reconstructed samples, as instructed in [README.md](https://github.com/ntuaislab/Trap-MID/blob/main/README.md). We used 5 runs for each experiments in our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy\n",
    "\n",
    "#### Option 1\n",
    "\n",
    "Create the json file to record the experiment and save the model accuracy with the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Model\": [\n",
    "        {\n",
    "            \"acc\": 0.8783\n",
    "        },\n",
    "        {\n",
    "            \"acc\": 0.8614\n",
    "        },\n",
    "        {\n",
    "            \"acc\": 0.8587\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "#### Option 2\n",
    "\n",
    "Use the code in following cells to extract model accuracy from training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'BEST' # BEST, LAST\n",
    "log_file = '<PATH_TO_TRAINING_LOG_FILE>'\n",
    "\n",
    "with open(log_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "if mode == 'BEST':\n",
    "    line = lines[-1].strip() # Last line\n",
    "    prefix = 'Best Acc:'\n",
    "elif mode == 'LAST':\n",
    "    line = lines[-2].strip() # Second last line\n",
    "    prefix = 'Test Acc:'\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "acc = float(line[line.find(prefix)+len(prefix):].split('|')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = '<PATH_TO_RESULT_FILE>'\n",
    "metric_name = 'Model'\n",
    "\n",
    "if os.path.exists(result_file):\n",
    "    with open(result_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = {}\n",
    "\n",
    "if metric_name not in results:\n",
    "    results[metric_name] = []\n",
    "results[metric_name].append({ \"acc\": acc / 100  })\n",
    "\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMI, KED-MI, and PLG-MI\n",
    "\n",
    "The following code shells shows the processing method of GMI, KED-MI, and PLG-MI results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = '<EXPERIMENT_NAME>'\n",
    "\n",
    "with open(f'<PATH_TO_ATTACK_RESULT>/{experiment}/result.json', 'r') as f:\n",
    "    result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = '<PATH_TO_RESULT_FILE>'\n",
    "attack_name = 'GMI' # GMI, KED-MI, PLG-MI\n",
    "\n",
    "if os.path.exists(result_file):\n",
    "    with open(result_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = {}\n",
    "\n",
    "if attack_name not in results:\n",
    "    results[attack_name] = []\n",
    "results[attack_name].append(result)\n",
    "\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOMMA\n",
    "\n",
    "The following code shells shows the processing method of LONMMA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = '<EXPERIMENT_NAME>'\n",
    "attack = 'gmi' # gmi, kedmi\n",
    "model = 'VGG16'\n",
    "\n",
    "df = pd.read_csv(f'<PATH_TO_LOMMA_RESULT>/{experiment}/{attack}_300ids/celeba_{model}/ours/Eval_results.csv')\n",
    "result = {\n",
    "    \"acc\": df['acc'][0] / 100,\n",
    "    \"acc5\": df['acc5'][0] / 100,\n",
    "    \"var\": (df['std'][0] / 100) ** 2,\n",
    "    \"var5\": (df['std5'][0] / 100) ** 2,\n",
    "    \"knn_dist\": df['knn'][0],\n",
    "    \"fid\": df['fid'][0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = '<PATH_TO_RESULT_FILE>'\n",
    "attack_name = 'LOMMA (GMI)' # LOMMA (GMI), LOMMA (KED-MI)\n",
    "\n",
    "if os.path.exists(result_file):\n",
    "    with open(result_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = {}\n",
    "\n",
    "if attack_name not in results:\n",
    "    results[attack_name] = []\n",
    "results[attack_name].append(result)\n",
    "\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
